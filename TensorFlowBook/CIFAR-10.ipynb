{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# git clone https://github.com/tensorflow/models.git\n",
    "# cd models/tutorials/image/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport cifar10, cifar10_input\\nimport tensorlfow as tf\\nimport numpy as np\\nimport time\\n\\nmax_steps  = 3000\\nbatch_size = 128\\ndata_dir = '/tmp/cifar10_data/cifar-10-batches-bin'\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import cifar10, cifar10_input\n",
    "import tensorlfow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "max_steps  = 3000\n",
    "batch_size = 128\n",
    "data_dir = '/tmp/cifar10_data/cifar-10-batches-bin'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef variable_with_weight_loss(shape, stddev, w1):\\n\\n    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\\n    if wl is not None:\\n        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name='weight_loss')\\n        tf.add_to_collection('losses', weight_loss)\\n    return var\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def variable_with_weight_loss(shape, stddev, w1):\n",
    "\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return var\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cifar10.maybe_dounload_and_extrac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# images_train, labels-=_train = cifar10_input.ditorted_inputs(\n",
    "#                                            data_dir=data_dir, barch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimages_test , labels_test = cifar10_input.inputs(eval_data=True, \\n                                                data_dir=data_dir, \\n                                                batch_size=batch_size)\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "images_test , labels_test = cifar10_input.inputs(eval_data=True, \n",
    "                                                data_dir=data_dir, \n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image_holder = tf.,placeholder(tf.float32, [batch_size, 24, 24, 3])\n",
    "# label_holder = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nweight1 = variabel_with_weight_loss(shape=[5, 5, 3, 64], stddev=5e-2, \\n                                        wl=0.0)\\nkernel1 = tf.nn.conv2d(image_holder, wiehgt1, [1, 1, 1, 1], padding='SAME')\\nbias1 = tf.nn.relu(tf.constant(0.0, shape=[64]))\\nconv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\\npool1 = tf.nn.max_poll(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], \\n                        padding='SAME')\\nnorm1 = tf.nn.lrn(pool1, 4, bias=1.0, bias=1.0, alpha=0.001 / 9.0 , beta=0.75)\\n\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weight1 = variabel_with_weight_loss(shape=[5, 5, 3, 64], stddev=5e-2, \n",
    "                                        wl=0.0)\n",
    "kernel1 = tf.nn.conv2d(image_holder, wiehgt1, [1, 1, 1, 1], padding='SAME')\n",
    "bias1 = tf.nn.relu(tf.constant(0.0, shape=[64]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\n",
    "pool1 = tf.nn.max_poll(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], \n",
    "                        padding='SAME')\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, bias=1.0, alpha=0.001 / 9.0 , beta=0.75)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nweight2 = variable_with_+weight_loss(shape-[5, 5, 64, 64], stddev=5e-2, \\n                                        wl=0.0)\\nkernel2 = tf.nn.conv2d(norm1, weight2, [1, 1, 1,1], padding='SAME')\\nbias2 = tf.Variable(tf.constant(0.1, shape=[64]))\\n\\nconv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\\nnorm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0 , beta=0.75)\\n\\npool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], \\n                        padding='SAME')\\n                        \\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weight2 = variable_with_+weight_loss(shape-[5, 5, 64, 64], stddev=5e-2, \n",
    "                                        wl=0.0)\n",
    "kernel2 = tf.nn.conv2d(norm1, weight2, [1, 1, 1,1], padding='SAME')\n",
    "bias2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
    "norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0 , beta=0.75)\n",
    "\n",
    "pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], \n",
    "                        padding='SAME')\n",
    "                        \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreshape = tf.reshape(pool2, [batch_size, -1])\\ndim  = reshape.get_shape()[1].value\\nweight3 = variable_with_weight_loss(shape=[dim, 384], stddev=0.04, wl=0.004)\\nbias3 = tf.Variable(tf.constant(0.1, shape=[384]))\\n\\nlocal3 = tf.nn.relu(tf.matmul(reshpae, wieght3) + bias3)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "dim  = reshape.get_shape()[1].value\n",
    "weight3 = variable_with_weight_loss(shape=[dim, 384], stddev=0.04, wl=0.004)\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "\n",
    "local3 = tf.nn.relu(tf.matmul(reshpae, wieght3) + bias3)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nweight4 = variabel_with_weight_loss(shape=[384, 192], stddev=0.04, wl=0.004)\\nbias4 = tf.Variable(tf.constant(0.1, shape=[192]))\\nlocal4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weight4 = variabel_with_weight_loss(shape=[384, 192], stddev=0.04, wl=0.004)\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape=[192]))\n",
    "local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nweight5 =varibale_with_weight_loss(shape=[192, 10], stddev=1/192.0, wl=0.0)\\nbias5 = tf.Variable(tf.constant(0.0, shape=[10]))\\nlogits = tf.add(tf.matmul(local4, weight5), bias5)\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weight5 =varibale_with_weight_loss(shape=[192, 10], stddev=1/192.0, wl=0.0)\n",
    "bias5 = tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "logits = tf.add(tf.matmul(local4, weight5), bias5)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef loss(logits, labels):\\n\\n    labels = tf.cast(labels, tf.int64)\\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\\n                    logits=logits, labels=labels, name='cross_entropy_per_example')\\n                    \\n    cross_entropy_mean = tf.reduce_mean(cross_entropy, \\n                                        name='cross_entropy')\\n    tf.add_to_collection('losses', cross_entropy_mean)\\n    \\n    return tf.add_n(tf.get_collection('losses') , name='total_loss')\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def loss(logits, labels):\n",
    "\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "                    \n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, \n",
    "                                        name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    \n",
    "    return tf.add_n(tf.get_collection('losses') , name='total_loss')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nloss = loss(logits, label_holder)\\ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\\ntop_k_op = tf.nn.in_top_k(logits, label_holder, 1)\\nsess = tf.InteractiveSession()\\ntf.global_variables_initializer().run()\\n\\ntf.train.start_queue_runners()\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "loss = loss(logits, label_holder)\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "top_k_op = tf.nn.in_top_k(logits, label_holder, 1)\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "tf.train.start_queue_runners()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor step in range(max_steps):\\n\\n    start_time = time.time()\\n    image_batch, label_batch = sess.run([images_train, labels_train])\\n    _, loss_value = sess.run([train_op, loss], \\n                        feed_dict={{iamge_holder: image_batch, label_holder:label_batch}})\\n                        \\n    duration = time().time() - start_time\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for step in range(max_steps):\n",
    "\n",
    "    start_time = time.time()\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    _, loss_value = sess.run([train_op, loss], \n",
    "                        feed_dict={{iamge_holder: image_batch, label_holder:label_batch}})\n",
    "                        \n",
    "    duration = time().time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif step %10 == 0 :\\n    exampels_per_sec = batch_size / duration \\n    sec_per_batch = float(duration)\\n    \\n    format_str = ('step %d, loss=%.2f (%%.1f examples/sec; %.3f sec/batch)')\\n    print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))\\n    \\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if step %10 == 0 :\n",
    "    exampels_per_sec = batch_size / duration \n",
    "    sec_per_batch = float(duration)\n",
    "    \n",
    "    format_str = ('step %d, loss=%.2f (%%.1f examples/sec; %.3f sec/batch)')\n",
    "    print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))\n",
    "    \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_examples = 1000\\nimport math\\nnum_iter = int(math.ceil(num_examples/ batch_size))\\ntrue _count = 0\\ntotal_sample_count = num_iter * batch_size\\nstep = 0\\nwhile step < num_iter:\\n    image_batch, label_batch = sess.run([images_test, labels_test])\\n    predictions = sess.run([top_k_op], feed_dict={image_holder:image_batch, \\n                                                    label_holder:label_batch})\\n    true_count += np.sum(predicitns)\\n    step += 1\\n    \\nprecision = true_cont / total / total_sample_count\\npritn('precision @1 =- %.3f'% precision)\\n\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_examples = 1000\n",
    "import math\n",
    "num_iter = int(math.ceil(num_examples/ batch_size))\n",
    "true _count = 0\n",
    "total_sample_count = num_iter * batch_size\n",
    "step = 0\n",
    "while step < num_iter:\n",
    "    image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "    predictions = sess.run([top_k_op], feed_dict={image_holder:image_batch, \n",
    "                                                    label_holder:label_batch})\n",
    "    true_count += np.sum(predicitns)\n",
    "    step += 1\n",
    "    \n",
    "precision = true_cont / total / total_sample_count\n",
    "pritn('precision @1 =- %.3f'% precision)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
